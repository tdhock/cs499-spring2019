Coding project 3: neural networks for regression and binary classification.

For this project you will be writing an R package 
that implements gradient descent algorithms for neural networks with one hidden layer.
You are allowed and encouraged to discuss your code/ideas with other students,
but it is strictly forbidden to copy any code/documentation/tests/etc from other groups,
or any web pages. Your code/project must be written from scratch by the members of your group. 
Any violations will result in one or more of the following: 
- a zero on this assignment, 
- a failing grade in this class,
- expulsion from NAU.

* Algorithms to code
By now you know how to code functions in C++ or R so you have the choice.
- if you code the algorithm in C++, write an R function that calls your C++ code via .C
- if you code the algorithm in pure R, use matrix/vector arithmetic
  for efficiency (do NOT use for loops over data points or features)

** Neural networks with early stopping regularization
R Function name: NNetIterations
- Inputs: X.mat (feature matrix, n_observations x n_features), y.vec
  (label vector, n_observations x 1), max.iterations (int scalar > 1),
  step.size, n.hidden.units (number of hidden units), is.train
  (logical vector of size n_observations, TRUE if the observation is
  in the train set, FALSE for the validation set).
- Output: list with named elements:
  - pred.mat, n_observations x max.iterations matrix of predicted
    values (real number for regression, probability for binary
    classification).
  - W.mat final weight matrix (n_features+1 x n.hidden.units)
  - v.vec final weight vector (n.hidden.units+1). 
  - predict(testX.mat), a function that takes a test features matrix
    and returns a vector of predictions (real numbers for regression,
    probabilities for binary classification). You should be able to
    make predictions via cbind(1, sigmoid(cbind(1, X) %*% W.mat)) %*%
    v.vec. The first row of W.mat should be the intercept terms; the
    first element of v.vec should be the intercept term.
- if all of y.vec is binary (either 0 or 1) then you should use the
  logistic loss, otherwise use the square loss.
- you should optimize the mean loss, which is the sum over all data
  divided by n_observations (if you don't divide by the number of
  observations the gradient can get numerically unstable for large
  data).
- make sure to compute a scaled input matrix, which has mean=0 and sd=1 for each column,
  and keep track of the mean/sd of each column, so you can return W.mat on the original scale
  (if you use the unscaled X.mat during gradient descent, it will not converge -- numerical instability).

R Function name: NNetEarlyStoppingCV
- Inputs: X.mat, y.vec, fold.vec, max.iterations, step.size, n.hidden.units.
- should use K-fold cross-validation based on the fold IDs provided in fold.vec
- for each train/validation split, use NNetIterations to compute the
  predictions for all observations
- compute mean.validation.loss.vec, which is a vector (with
  max.iterations elements) of mean validation loss over all K
  folds (use the square loss for regression and the 01-loss for
  binary classification).
- compute mean.train.loss.vec, analogous to above but for the train data.
- minimize the mean validation loss to determine selected.steps, the
  optimal number of steps/iterations.
- finally use NNetIterations(max.iterations=selected.steps) on the whole training data set.
- Output the same list from NNetIterations and add
  - mean.validation.loss, mean.train.loss.vec (for plotting train/validation loss curves)
  - selected.steps
  
** Documentation and tests
- for each R function, write documentation with at least one example of how to use it.
- write at least two tests for each R function, in tests/testthat/test-xxx.R.
    You should at least test that 
    (1) for valid inputs your function returns an output of the expected type/dimension, 
    (2) for an invalid input, your function stops with an informative error message.
    
* Experiments/application: run your code on the following data sets.
- Binary classification
  - ElemStatLearn::spam 2-class [4601, 57] output is last column (spam).
  - ElemStatLearn::SAheart 2-class [462, 9] output is last column (chd).
  - ElemStatLearn::zip.train: 10-class [7291, 256] output is first column. (ignore classes other than 0 and 1)
- Regression
  - ElemStatLearn::prostate [97 x 8] output is lpsa column, ignore train column.
  - ElemStatLearn::ozone [111 x 3] output is first column (ozone).
- For each data set, use 5-fold cross-validation to evaluate the
  prediction accuracy of your code. For each split, set
  aside the data in fold s as a test set.  Use NNetEarlyStoppingCV to train a model
  on the other folds (which should be used in your function as
  internal train/validation sets/splits), then make a prediction on
  the test fold s.
- For each train/test split, 
  to show that your algorithm is actually learning something 
  non-trivial from the inputs/features,
  compute a baseline predictor that ignores the inputs/features.
  - Regression: the mean of the training labels/outputs.
  - Binary classification: the most frequent class/label/output in the training data.
- For each data set, compute a s x 2 matrix of mean test loss values:
  - each of the rows are for a specific test set,
  - the first column is for the neural network predictor,
  - the second column is for the baseline/un-informed predictor.
- Make one or more plot(s) or table(s) that compares these test loss values. 
  For each of the five data sets, 
  is the neural network more accurate than the baseline?
- for each data set, run NNetEarlyStoppingCV functions on the entire data set,
  and plot the mean validation loss as a function of the regularization parameter. 
  plot the mean train loss in one color, and the mean validation loss in another color.
  Plot a point and/or text label to emphasize the regularization parameter
  selected by minimizing the mean validation loss function.
- Write up your results in vignettes/report.Rmd that shows the R code that you used
  for the experiments/application, along with the output. 
  - Documentation: [[http://r-pkgs.had.co.nz/vignettes.html][Vignettes chapter of R packages book]].
  - Example [[https://github.com/cran/glmnet/blob/master/vignettes/glmnet_beta.Rmd][Rmd vignette source code]].
    [[https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html][vignette rendered to HTML]].
  - For this assignment the headings should be as follows:

#+BEGIN_SRC
## Data set 1: spam

### Matrix of loss values

print out and/or plot the matrix.

comment on difference in accuracy.

### Train/validation loss plots

plot the two loss functions.

What are the optimal regularization parameters?

## Data set 2: ...
#+END_SRC

* Grading rubric: 100 points.
Your group should submit a link to your repo on GitHub.
- 20 points for completeness of report.
  - 4 points for each data set (2 points each for loss matrix and train/validation loss plot)
- 20 points if your R package passes with no WARNING/ERROR on
  https://win-builder.r-project.org/
  - minus 5 points for every WARNING/ERROR.
- 20 points for group evaluations -- this is to make sure that each group member participates more or less equally. You will get points deducted if your fellow group members give you a bad evaluation.
- 20 points for accuracy of your code (I will run tests to make sure
  your functions give errors for bad inputs, and the proper output for
  good inputs).
- 10 points for R documentation pages.
  - 5 points for informative example code.
  - 5 points for documenting types/dimensions of inputs/outputs.
- 10 points for tests, as described above.

Extra credit: 
  - 1-30 points extra credit if, in your Rmd report, you also compare
    against NNLearnCV/LM__L2CV/LM__EarlyStoppingCV, and comment on
    whether or not the neural network is more accurate (max 3 points
    per data set, one point for each algo). Note that the only way to
    get this to work along with CRAN/win-builder checks is by copying
    the code from the previous R package(s) to your package for
    project 3.
  - 10 points extra credit if, in your Rmd report, you use LaTeX
    code/MathJax to type the equations for the loss, gradient, and
    backpropagation updates.
  - 10 points if, in your GitHub repo, you setup Travis-CI to check
    your R package, and have a green badge that indicates a build that
    passes checks.  See [[https://juliasilge.com/blog/beginners-guide-to-travis/][blog]] and [[https://docs.travis-ci.com/user/languages/r/][docs]].
  - if you submit your work early (to me via email) you will get
    feedback from me and extra credit:
    - First week: 10 points if you have written both functions
      described above, and you email me with a link to your github
      repo by Tuesday Apr 2. (1 point per function)
    - Second week: 10 more points if you have started your report, and
      you email me with the rendered HTML report as an attachment by
      Tue Apr 9.  You will get 2 points of extra credit for the
      analysis of each data set (1 point for plots of train/validation
      loss versus regularization parameter, 1 point for 4-fold CV loss
      matrix table/plot).
    - Third week: do tests/docs, finish report, make sure package
      passes R CMD check with no WARNING/ERROR on win-builder, send me
      a link to your github project via email by Fri Apr 12.

** FAQ

