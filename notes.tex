\documentclass{article}

\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fullpage}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

Each section is a new lecture.

\section{Applications}
\begin{itemize}
\item slides on applications of machine learning.
\item calculus/programming quiz.
\end{itemize}

\section{Training data in supervised learning}

Training data: $D= \{(x_1,y_1), \dots, (x_n,y_n)\}$, given/known.

Inputs $x_i\in\mathcal X$, outputs
$y_i\in\mathcal y$.

For each example, what is $\mathcal X,\mathcal Y$? 
\begin{itemize}
\item Image classification
\item Image segmntation
\item Translation
\item Spam filtering
\end{itemize}

Usually $\mathcal X = \mathbb R^p$ (e.g. word counts in emails).

Notation. 
\begin{itemize}
\item Inputs $x\in\mathbb R^p$.
\item $p$ = dimension of each input.
\item $n$ = number of training examples.
\item Outputs $y\in\mathbb R$ for regression, $y\in\{0,1\}$ for binary
  classification.
\item $f:\mathcal X\rightarrow \mathcal Y$ is the prediction function
  we want to learn.
\item Regression function $f:\mathbb R^p\rightarrow\mathbb R$
\item Binary classification function $f:\mathbb R^p\rightarrow \{0,1\}$.
\end{itemize}

Geometric interpretation of regression, height son/father (linear
pattern), species versus temperature (non-linear pattern).

Draw residual $r_i$ on graphs as vertical line segments.

\paragraph{Test data.} $D'= \{ (x'_1,y'_1), \dots, (x'_m,y'_m)\}$, unknown.
Goal is generalization to new/test data: algo $\textsc{Learn}(D) = f$,
with $f(x'_i)\approx y'_i$ for all test data, i.e. minimize
\begin{equation}
  \text{Err}_{D'}(f) = \sum_{(x',y')\in D'} \ell[f(x'), y']
\end{equation}
where $\ell$ is a loss function:
\begin{itemize}
\item In binary classification we typically use the
  mis-classification-rate/0-1-loss $\ell[f(x),y]=I[c_f(x)!=y]$, where
  the binary classifier $c_f(x)=0$ if $f(x)<0.5$ and 1 otherwise.
\item In regression we use the squared error $\ell[f(x),y]=[f(x)-y]^2$.
\end{itemize}
Since $D'$ is unknown, we need to assume that $D,D'\sim \mathcal P$.

\paragraph{Nearest neighbors algorithm.} What is near?
$\textsc{Dist}:\mathcal X\times \mathcal X \rightarrow \mathbb R_+$ is
a non-negative distance function between two data points.

e.g. for $\mathcal X=\mathbb R^p$ we use
\begin{itemize}
\item L1 distance $\textsc{Dist}(x,x') = ||x-x'||_1 = \sum_{j=1}^p |x_j-x_j'|$
\item L2 distance $\textsc{Dist}(x,x') = ||x-x'||_2 = \sqrt{
\sum_{j=1}^p (x_j-x_j')^2}$
\end{itemize}
Draw geometric interpretation of L1 and L2 distances.

Compute distances $d_1,\dots,d_n\in\mathbb R_+$ where 
$d_i=\textsc{Dist}(x_i,x)$.

Example graph, draw horizontal line segments $d_i$ between $x,x_i$.

Define neighbors function $N_{D,K}(x') = \{t_1, t_2, \dots, t_K\}$, where\\
$t_1,\dots,t_n\in\{1,\dots,n\}$ are indices such that\\
distances $d_{t_1}\leq \cdots \leq d_{t_n}$ are sorted ascending.

Predict the mean output value of the $K$ nearest
neighbors, $$f_{D,K}(x') = \frac 1 K \sum_{i\in N_{D,K}(x')} y_i$$

\begin{itemize}
\item For regression this is the mean.
\item For binary classification the mean is intepreted as a
  probability, predict 1 if greater than 0.5, and predict 0 otherwise.
\end{itemize}

\section{Nearest neighbors model selection and code}
\begin{itemize}
\item Review of supervised learning.
\item Definition of nearest neighbors prediction function $f_{D,K}(x)$.  
\item How to choose $K$? Discussion.
\item Formal definition of total error.
\item Visualizations. 1d regression and binary classification.
\item Draw train/validation matrices. Validation error.
\item Pseudocode for computing $f_{D,K}(x)$.
\end{itemize}

\begin{equation}
  \mathcal L_{D,D'}(k) = \sum_{(x',y')\in D'} \ell[f_{D,k}(x'), y']
\end{equation}

\begin{algorithmic}[1]
  \State Function $\textsc{PredKNearestNeighbors}$
  \State Inputs: train inputs/features $x_1,\dots,x_n$, outputs/labels $y_1,\dots,y_n$,\\ \ \ \ test input/feature $x'$, max number of neighbors $K_{\text{max}}$:
  \For{$i=1$ to $n$}:
  \State $d_i\gets \textsc{Distance}(x', x_i)$
  \EndFor
  \State $t_1,\dots,t_n\gets \textsc{SortedIndices}(d_1,\dots,d_n)$
  \State $\text{totalY}\gets 0.0$
  \For{$k=1$ to $K_{\text{max}}$}:
  \State $i\gets t_k$
  \State $\text{totalY} \texttt{ += } y_i$
  \EndFor
  \State Output: prediction $\text{totalY}/K_{\text{max}}$
\end{algorithmic}
Total complexity: $O(np + n\log n)$ where the Distance sub-routine is $O(p)$.

\section{Cross-validation}

Interactive data viz.

Remember we assume that training data and test data are randomly drawn
from a common probability distribution $\mathcal P$. To simulate that
process, we randomly assign a fold ID for each observation
$z_1,\dots,z_n\in\{1,\dots, S_\text{max}\}$. We then define
$\text{NumFolds}$ train/validation splits
\begin{equation}
  D_{z=s} = \{(x_i,y_i)|z_i = s\} \text{ --- validation set $s$}
  D_{z\neq s} = \{(x_i,y_i)|z_i \neq s\} \text{ --- train set $s$}
\end{equation}
Draw train/validation/test matrices for $s=1$, $s=2$, etc.

Cross-validation data visualizations.

Mean validation error over all train/validation splits (folds):
\begin{equation}
  \text{MeanVerr}(k) = \mathcal L_{D_{z\neq s},D_{z=s}}(k)
\end{equation}

Overall we choose the parameter $\hat k = \argmin_k \text{MeanVerr}(k)$.

And the learning algorithm is $\textsc{LearnKNN}(D) = f_{D, \hat k}$.

\subsection{Pseudocode}

We assume there are sub-routines
\begin{itemize}
\item
  $\textsc{Pred1toKmaxNN}(D, x', K_{\text{max}}) = [ f_{D, 1}(x')
  ... f_{D, K_{\text{max}}}(x') ]$ -- all predictions from $k=1$ to
  $K_{\text{max}}$ neighbors for a single test point $x'$ based on the training
  data set $D$.
\item $\textsc{PredError}(D, D', K_{\text{max}})$ trains on $D$ and
  computes $\mathcal L_{D,D'}(k)$, the validation error wrt $D'$ for
  $k=1$ to $K_{\text{max}}$ neighbors.
\end{itemize}

Then the pseudo code for the learning algo is:

\begin{algorithmic}[1]
  \State Function $\textsc{LearnKNN}$
  \State Inputs: train data $D$ ($n$ observations in $p$ dimensions), 
  number of folds $S_{\text{max}}\in\{1,\dots, n\}$, 
  max number of neighbors $K_{\text{max}}\in\{1, \dots, n\}$.
  \State Randomly assign fold IDs $z\in\{1,\dots,S_{\text{max}}\}^n$
  \For{each fold $s=1$ to $S_{\text{max}}$}:
  \State $E_s\gets \textsc{PredError}(D_{z\neq s}, D_{z=s}, K_{\text{max}})\in\mathbb R^{K_{\text{max}}}$
  \EndFor
  Let
  $E\gets[E_1 ... E_{S_{\text{max}}}]\in\mathbb R^{K_{\text{max}}\times
    S_{\text{max}}}$ be the matrix of error/loss values for all
  folds/columns and neighbors/rows.
  \State $\text{MeanVerr}\gets \textsc{colMeans}(E)\in\mathbb R^{K_{\text{max}}}$
  \State The optimal number of neighbors is $\hat k = \argmin_k \text{MeanVerr}_k$
  \State return $f_{D,\hat k}$
\end{algorithmic}

\section{R package development}

Choose groups.

\section{Coding nearest neighbors prediction}

For one test point, demo code in C++.

\section{Linear regression / gradient descent}

Want to minimize squared error on test data, but we don't have them,
so instead we do
\begin{equation}
  \min_f \sum_{i=1}^n [f(x_i)-y_i]^2
\end{equation}
We parameterize a linear function using a weight vector $w\in\mathbb R^p$,
$f_w(x_i)=w^T x_i$. 

The input feature matrix is 
\begin{equation}
  X = \left[\begin{array}{c}
              x_1^T \\
              \vdots \\
              x_n^T
\end{array}\right]\in\mathbb R^{n\times p}
\end{equation}
So for a partcular weight vector $w$ the prediction vector is
\begin{equation}
  Xw = \left[\begin{array}{c}
              x_1^T w \\
              \vdots \\
              x_n^T w
\end{array}\right]\in\mathbb R^{n}
\end{equation}
So the least squares regression problem is
\begin{equation}
  \min_{w\in\mathbb R^p} || Xw - y||_2^2
\end{equation}
Notation about matrix/vector dimensions.

What is a norm $||\cdot||$ = size of a vector.

Squared L2 norm $||v||_2^2 = v^T v$.

L1 norm $||v||_1 = \sum_{j=1}^p |v_j|$.

Univariate linear regression: $p=2$, $x_i=[1\ x_{i,2}]$.
\begin{equation}
  X = \left[\begin{array}{cc}
              1 & x_{1,2} \\
              \vdots & \vdots \\
              1 & x_{n,2}
\end{array}\right]\in\mathbb R^{n\times 2}
\end{equation}
and
$f(x_i) = w^T x_i = \underline{w_1}_{\text{intercept}} +
\underline{w_2}_{\text{slope}} x_{i,2}$. Draw figure on board then
show data viz.

Gradient is defined for any $w$ as the direction of steepest ascent:
\begin{equation}
  \nabla \mathcal L(w) = \left[\begin{array}{c}
              \frac{d}{dw_1} \mathcal L(w)\\
              \vdots \\
              \frac{d}{dw_p} \mathcal L(w)\\
\end{array}\right]\in\mathbb R^{p}
\end{equation}
For $p=1$ we have 
\begin{equation}
  \nabla \mathcal L(w) = \mathcal L'(w) = \frac{d}{dw}\mathcal L(w) = 
  \sum_{i=1}^n 2x_i(w x_i - y_i).
\end{equation}
In general we have
\begin{equation}
  \nabla \mathcal L(w) = 
  \nabla_w ||Xw - y||_2^2 = 
  2X^T(Xw-y).
\end{equation}
Algorithm: $w^{(0)}=0$ be the initial guess.
Then for any stepsize $\alpha>0$ define update rules for any iteration $t>0$:
\begin{equation}
  w^{(t)} = w^{(t-1)} - \alpha \nabla \mathcal L(w^{(t-1)}).
\end{equation}
Example: gradient descent on $\mathcal L(w)=(x+5)^2$. Start at the
origin. Compute gradient, first step.


\end{document}
